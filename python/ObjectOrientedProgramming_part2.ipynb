{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngineBase(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def add_corpus(self, file_path):\n",
    "        with open(file_path, 'r') as fin:\n",
    "            text = fin.read()\n",
    "        self.process_corpus(file_path, text)\n",
    "\n",
    "    def process_corpus(self, id, text):\n",
    "        raise Exception('process_corpus not implemented.')\n",
    "\n",
    "    def search(self, query):\n",
    "        raise Exception('search not implemented.')\n",
    "\n",
    "def main(search_engine):\n",
    "\n",
    "    for file_path in ['1.txt', '2.txt', '3.txt', '4.txt', '5.txt']:\n",
    "        search_engine.add_corpus(fr'e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\{file_path}')\n",
    "\n",
    "    while True:\n",
    "        query = input()\n",
    "        results = search_engine.search(query)\n",
    "        print('found {} result(s):'.format(len(results)))\n",
    "        for result in results:\n",
    "            print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 0 result(s):\n",
      "found 2 result(s):\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\1.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\2.txt\n",
      "found 0 result(s):\n",
      "found 5 result(s):\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\1.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\2.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\3.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\4.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\5.txt\n",
      "found 5 result(s):\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\1.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\2.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\3.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\4.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\5.txt\n",
      "found 5 result(s):\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\1.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\2.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\3.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\4.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\5.txt\n",
      "found 5 result(s):\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\1.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\2.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\3.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\4.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\5.txt\n",
      "found 0 result(s):\n",
      "found 0 result(s):\n"
     ]
    }
   ],
   "source": [
    "class SimpleEngine(SearchEngineBase):\n",
    "    def __init__(self):\n",
    "        super(SimpleEngine, self).__init__()\n",
    "        self.__id_to_texts = {}\n",
    "\n",
    "    def process_corpus(self, id, text):\n",
    "        self.__id_to_texts[id] = text\n",
    "\n",
    "    def search(self, query):\n",
    "        results = []\n",
    "        for id, text in self.__id_to_texts.items():\n",
    "            if query in text:\n",
    "                results.append(id)\n",
    "        return results\n",
    "\n",
    "search_engine = SimpleEngine()\n",
    "main(search_engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SimpleEngine` 实现了一个继承 `SearchEngineBase` 的子类，继承并实现了 `process_corpus` 和 `search` 接口，同时，也顺手继承了 `add_corpus` 函数。\n",
    "\n",
    "在我们新的构造函数中，`self.__id_to_texts = {}` 初始化了自己的私有变量，也就是这个用来存储文件名到文件内容的字典。\n",
    "\n",
    "`process_corpus()` 函数则非常直白地将文件内容插入到字典中。这里注意，ID 需要是唯一的，不然相同 ID 的新内容会覆盖掉旧的内容。\n",
    "\n",
    "`search` 直接枚举字典，从中找到要搜索的字符串。如果能够找到，则将 ID 放到结果列表中，最后返回。\n",
    "\n",
    "1. 现在你对父类子类的构造函数调用顺序和方法应该更清楚了吧？ \n",
    "2. 继承的时候，函数是如何重写的？ \n",
    "3. 基类是如何充当接口作用的（你可以自行删掉子类中的重写函数，抑或是修改一下函数的参数，看一下会报什么错）？ \n",
    "4. 方法和变量之间又如何衔接起来的呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相信你也能看得出来，这种实现方式简单，但显然是一种很低效的方式：每次索引后需要占用大量空间，因为索引函数并没有做任何事情；每次检索需要占用大量时间，因为所有索引库的文件都要被重新搜索一遍。如果把语料的信息量视为 n，那么这里的时间复杂度和空间复杂度都应该是 O(n) 级别的。 \n",
    "\n",
    "而且，还有一个问题：这里的 query 只能是一个词，或者是连起来的几个词。如果你想要搜索多个词，它们又分散在文章的不同位置，我们的简单引擎就无能为力了。 \n",
    "\n",
    "这时应该怎么优化呢？ \n",
    "\n",
    "最直接的一个想法，就是把语料分词，看成一个个的词汇，这样就只需要对每篇文章存储它所有词汇的 set 即可。根据齐夫定律（Zipf’s law，https://en.wikipedia.org/wiki/Zipf%27s_law），在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比，呈现幂律分布。因此，语料分词的做法可以大大提升我们的存储和搜索效率。 \n",
    "\n",
    "那具体该如何实现呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words 和 Inverted Index\n",
    "\n",
    "我们先来实现一个名叫 Bag of Words 的搜索模型。请看下面的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 5 result(s):\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\1.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\2.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\3.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\4.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\5.txt\n",
      "found 3 result(s):\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\1.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\2.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\3.txt\n",
      "found 1 result(s):\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\5.txt\n",
      "found 0 result(s):\n",
      "found 5 result(s):\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\1.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\2.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\3.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\4.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\5.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class BOWEngine(SearchEngineBase):\n",
    "    def __init__(self):\n",
    "        super(BOWEngine, self).__init__()\n",
    "        self.__id_to_words = {}\n",
    "\n",
    "    def process_corpus(self, id, text):\n",
    "        self.__id_to_words[id] = self.parse_text_to_words(text)\n",
    "\n",
    "    def search(self, query):\n",
    "        query_words = self.parse_text_to_words(query)\n",
    "        results = []\n",
    "        for id, words in self.__id_to_words.items():\n",
    "            if self.query_match(query_words, words):\n",
    "                results.append(id)\n",
    "        return results\n",
    "    \n",
    "    @staticmethod\n",
    "    def query_match(query_words, words):\n",
    "        for query_word in query_words:\n",
    "            if query_word not in words:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_text_to_words(text):\n",
    "        # 使用正则表达式去除标点符号和换行符\n",
    "        text = re.sub(r'[^\\w ]', ' ', text)\n",
    "        # 转为小写\n",
    "        text = text.lower()\n",
    "        # 生成所有单词的列表\n",
    "        word_list = text.split(' ')\n",
    "        # 去除空白单词\n",
    "        word_list = filter(None, word_list)\n",
    "        # 返回单词的 set\n",
    "        return set(word_list)\n",
    "\n",
    "search_engine = BOWEngine()\n",
    "main(search_engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW Model，即 Bag of words Model, 中文叫做词袋模型。 \n",
    "\n",
    "假设有一个文本，不考虑语法，句法，段落，也不考虑词汇出现的顺序，只将这个文本看成这些词汇的集合。所以在代码中将原有的`id_to_texts`替换成`id_to_words`，这样就只需要存这些单词，而不是全部文章，也不需要考虑顺序。其中， `process_corpus()` 函数调用类静态函数 `parse_text_to_words`，将文章打碎形成词袋，放入 `set` 之后再放到字典中。\n",
    "\n",
    "`search()` 函数则稍微复杂一些。这里我们假设，想得到的结果，是所有的搜索关键词都要出现在同一篇文章中。那么，我们需要同样打碎 query 得到一个 set，然后把 set 中的每一个词，和我们的索引中每一篇文章进行核对，看一下要找的词是否在其中。而这个过程由静态函数 `query_match` 负责。\n",
    "\n",
    "这样的解决方案存在一些问题：\n",
    "1. 每次查询时依然需要遍历所有 ID，虽然比起 Simple 模型已经节约了大量时间，但是互联网上有上亿个页面，每次都全部遍历的代价还是太大了\n",
    "2. 词袋模型并不考虑单词间的顺序，但有些人希望单词按顺序出现，或者希望搜索的单词在文中离得近一些，这种情况下词袋模型现任就无能为力了\n",
    "\n",
    "针对以上两点可以做出以下改进："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2 result(s):\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\1.txt\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\2.txt\n",
      "found 1 result(s):\n",
      "e:\\git\\learning\\python\\materials\\ObjectOrientedProgramming_p2\\2.txt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(word_list)\n\u001b[0;32m     66\u001b[0m search_engine \u001b[38;5;241m=\u001b[39m BOWInvertedIndexEngine()\n\u001b[1;32m---> 67\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_engine\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(search_engine)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m()\n\u001b[1;32m---> 23\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msearch_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfound \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m result(s):\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(results)))\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[1;32mIn[5], line 44\u001b[0m, in \u001b[0;36mBOWInvertedIndexEngine.search\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 然后，如果 current_ids 的所有元素都一样，那么表明这个单词在这个元素对应的文档中都出现了\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(x \u001b[38;5;241m==\u001b[39m current_ids[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m current_ids):\n\u001b[1;32m---> 44\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcurrent_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     45\u001b[0m     query_words_index \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m query_words_index]\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class BOWInvertedIndexEngine(SearchEngineBase):\n",
    "    def __init__(self):\n",
    "        super(BOWInvertedIndexEngine, self).__init__()\n",
    "        self.inverted_index = {}\n",
    "\n",
    "    def process_corpus(self, id, text):\n",
    "        words = self.parse_text_to_words(text)\n",
    "        for word in words:\n",
    "            if word not in self.inverted_index:\n",
    "                self.inverted_index[word] = []\n",
    "            self.inverted_index[word].append(id)\n",
    "\n",
    "    def search(self, query):\n",
    "        query_words = list(self.parse_text_to_words(query))\n",
    "        query_words_index = list()\n",
    "        for query_word in query_words:\n",
    "            query_words_index.append(0)\n",
    "        \n",
    "        # 如果某一个查询单词的倒序索引为空，我们就立刻返回\n",
    "        for query_word in query_words:\n",
    "            if query_word not in self.inverted_index:\n",
    "                return []\n",
    "        \n",
    "        result = []\n",
    "        while True:\n",
    "            \n",
    "            # 首先，获得当前状态下所有倒序索引的 index\n",
    "            current_ids = []\n",
    "            \n",
    "            for idx, query_word in enumerate(query_words):\n",
    "                current_index = query_words_index[idx]\n",
    "                current_inverted_list = self.inverted_index[query_word]\n",
    "                \n",
    "                # 已经遍历到了某一个倒序索引的末尾，结束 search\n",
    "                if current_index >= len(current_inverted_list):\n",
    "                    return result\n",
    "\n",
    "                current_ids.append(current_inverted_list[current_index])\n",
    "\n",
    "            # 然后，如果 current_ids 的所有元素都一样，那么表明这个单词在这个元素对应的文档中都出现了\n",
    "            if all(x == current_ids[0] for x in current_ids):\n",
    "                result.append(current_ids[0])\n",
    "                query_words_index = [x + 1 for x in query_words_index]\n",
    "                continue\n",
    "            \n",
    "            # 如果不是，我们就把最小的元素加一\n",
    "            min_val = min(current_ids)\n",
    "            min_val_pos = current_ids.index(min_val)\n",
    "            query_words_index[min_val_pos] += 1\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_text_to_words(text):\n",
    "        # 使用正则表达式去除标点符号和换行符\n",
    "        text = re.sub(r'[^\\w ]', ' ', text)\n",
    "        # 转为小写\n",
    "        text = text.lower()\n",
    "        # 生成所有单词的列表\n",
    "        word_list = text.split(' ')\n",
    "        # 去除空白单词\n",
    "        word_list = filter(None, word_list)\n",
    "        # 返回单词的 set\n",
    "        return set(word_list)\n",
    "\n",
    "search_engine = BOWInvertedIndexEngine()\n",
    "main(search_engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新模型继续使用之前的接口，仍然只在 `__init__()`、`process_corpus()`和`search()`三个函数进行修改。这其实也是大公司里团队协作的一种方式，在合理的分层设计后，每一层的逻辑只需要处理好分内的事情即可。在迭代升级我们的搜索引擎内核时， main 函数、用户接口没有任何改变。当然，如果公司招了新的前端工程师，要对用户接口部分进行修改，新人也不需要过分担心后台的事情，只要做好数据交互就可以了。\n",
    "\n",
    "继续看代码，你可能注意到了开头的 Inverted Index。Inverted Index Model，即倒序索引，是非常有名的搜索引擎方法，接下来我简单介绍一下。\n",
    "\n",
    "倒序索引，故如其名，也就是说这次反过来，我们保留的是 word -> id 的字典。于是情况就豁然开朗了，在 search 时，我们只需要把想要的 query_word 的几个倒序索引单独拎出来，然后从这几个列表中找共有的元素，那些共有的元素，即 ID，就是我们想要的查询结果。这样，我们就避免了将所有的 index 过一遍的尴尬。\n",
    "\n",
    "process_corpus 建立倒序索引。注意，这里的代码都是非常精简的。在工业界领域，需要一个 unique ID 生成器，来对每一篇文章标记上不同的 ID，倒序索引也应该按照这个 unique_id 来进行排序。\n",
    "\n",
    "至于 search() 函数，你大概了解它做的事情即可。它会根据 query_words 拿到所有的倒序索引，如果拿不到，就表示有的 query word 不存在于任何文章中，直接返回空；拿到之后，运行一个“合并 K 个有序数组”的算法，从中拿到我们想要的 ID，并返回。\n",
    "\n",
    "注意，这里用到的算法并不是最优的，最优的写法需要用最小堆来存储 index。这是一道有名的 leetcode hard 题，有兴趣请参考：https://blog.csdn.net/qqxx6661/article/details/77814794）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遍历的问题解决了，那第二个问题，如果我们想要实现搜索单词按顺序出现，或者希望搜索的单词在文中离得近一些呢？ 我们需要在 Inverted Index 上，对于每篇文章也保留单词的位置信息，这样一来，在合并操作的时候处理一下就可以了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRU和多重继承\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这一步，终于，你的搜索引擎上线了，有了越来越多的访问量（QPS）。欣喜骄傲的同时，你却发现服务器有些“不堪重负”了。经过一段时间的调研，你发现大量重复性搜索占据了 90% 以上的流量，于是，你想到了一个大杀器——给搜索引擎加一个缓存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylru\n",
    "\n",
    "class LRUCache(object):\n",
    "    def __init__(self, size=32):\n",
    "        self.cache = pylru.lrucache(size)\n",
    "    \n",
    "    def has(self, key):\n",
    "        return key in self.cache\n",
    "    \n",
    "    def get(self, key):\n",
    "        return self.cache[key]\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        self.cache[key] = value\n",
    "\n",
    "class BOWInvertedIndexEngineWithCache(BOWInvertedIndexEngine, LRUCache):\n",
    "    def __init__(self):\n",
    "        super(BOWInvertedIndexEngineWithCache, self).__init__()\n",
    "        LRUCache.__init__(self)\n",
    "    \n",
    "    def search(self, query):\n",
    "        if self.has(query):\n",
    "            print('cache hit!')\n",
    "            return self.get(query)\n",
    "        \n",
    "        result = super(BOWInvertedIndexEngineWithCache, self).search(query)\n",
    "        self.set(query, result)\n",
    "        \n",
    "        return result\n",
    "\n",
    "search_engine = BOWInvertedIndexEngineWithCache()\n",
    "main(search_engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它的代码很简单，LRUCache 定义了一个缓存类，你可以通过继承这个类来调用其方法。LRU 缓存是一种很经典的缓存（同时，LRU 的实现也是硅谷大厂常考的算法面试题，这里为了简单，我直接使用 pylru 这个包），它符合自然界的局部性原理，可以保留最近使用过的对象，而逐渐淘汰掉很久没有被用过的对象。\n",
    "\n",
    "这里的缓存使用起来也很简单，调用 has() 函数判断是否在缓存中，如果在，调用 get 函数直接返回结果；如果不在，送入后台计算结果，然后再塞入缓存。\n",
    "\n",
    "BOWInvertedIndexEngineWithCache 类，多重继承了两个类。多重继承有两种：\n",
    "\n",
    "第一种方法，用下面这行代码，直接初始化该类的第一个父类：`super(BOWInvertedIndexEngineWithCache, self).__init__()`,不过使用这种方法时，要求继承链的最顶层父类必须要继承 object。\n",
    "\n",
    "\n",
    "第二种方法，对于多重继承，如果有多个构造函数需要调用， 我们必须用传统的方法L`RUCache.__init__(self)` 。你应该注意，search() 函数被子类 BOWInvertedIndexEngineWithCache 再次重载，但是我还需要调用 BOWInvertedIndexEngine 的 search() 函数，这时该怎么办呢？请看下面这行代码：`super(BOWInvertedIndexEngineWithCache, self).search(query)`。我们可以强行调用被覆盖的父类的函数。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
